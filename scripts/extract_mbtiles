#!/usr/bin/env python3

from pathlib import Path
from PIL import Image
import argparse
import sqlite3
import hashlib
import logging
import signal
import time
import math
import json
import sys
import io


# ========= LOGS =========
LOGGER = logging.getLogger(__name__)
handler = logging.StreamHandler()
handler.setFormatter(logging.Formatter("%(asctime)s %(levelname)5s: %(message)s"))
LOGGER.addHandler(handler)
LOGGER.setLevel(logging.INFO)


SPHERICAL_RADIUS = 6378137.0
MAX_LON = 180
MAX_LAT = 85.051129


def limit_value(value, min = None, max = None):
	if min is not None and value < min:
		value = min

	if max is not None and value > max:
		value = max

	return value


def lonlat4326_to_xy3857(lon, lat):
	return limit_value(lon, -MAX_LON, MAX_LON) * math.pi / 180 * SPHERICAL_RADIUS, math.log(math.tan(math.pi * (limit_value(lat, -MAX_LAT, MAX_LAT) + 90) / 360)) * SPHERICAL_RADIUS


def xy3857_to_lonlat4326(x, y):
	return limit_value(x / SPHERICAL_RADIUS * 180 / math.pi, -MAX_LON, MAX_LON), limit_value(math.atan(math.sinh(y / SPHERICAL_RADIUS)) * 180 / math.pi, -MAX_LAT, MAX_LAT)


def get_xyz_from_lonlatz(lon, lat, z, scheme="tms"):
	max_tile = 1 << z

	x = (0.5 + limit_value(lon, -MAX_LON, MAX_LON) / 360) * max_tile
	y = (0.5 - math.log(math.tan(math.pi * (limit_value(lat, -MAX_LAT, MAX_LAT) + 90) / 360)) / (2 * math.pi)) * max_tile

	if scheme == "tms":
		y = max_tile - y

	return [limit_value(math.floor(x), 0, max_tile - 1), limit_value(math.floor(y), 0, max_tile - 1), z]


def get_lonlat_from_xyz(x, y, z, position, scheme="tms"):
	max_tile = 1 << z

	if scheme == "tms":
		y = max_tile - 1 - y

	if position == "center":
		x += 0.5
		y += 0.5
	elif position == "bottom-right":
		x += 1
		y += 1

	return [360 * (x / max_tile - 0.5), (360 * math.atan(math.exp(math.pi * (1 - 2 * y / max_tile))) / math.pi) - 90]


def get_bbox_from_tiles(x_min, y_min, x_max, y_max, z, scheme = "tms"):
	[lon_min, lat_max] = get_lonlat_from_xyz(x_min, y_min, z, "top-left", scheme)
	[lon_max, lat_min] = get_lonlat_from_xyz(x_max, y_max, z, "bottom-right", scheme)

	if lon_min > lon_max:
		[lon_min, lon_max] = [lon_max, lon_min]

	if lat_min > lat_max:
		[lat_min, lat_max] = [lat_max, lat_min]

	return [lon_min, lat_min, lon_max, lat_max]


def get_tiles_from_bbox(bbox, z, scheme = "tms"):
	[x_min, y_min] = get_xyz_from_lonlatz(bbox[0], bbox[3], z, scheme)
	[x_max, y_max] = get_xyz_from_lonlatz(bbox[2], bbox[1], z, scheme)

	if x_min > x_max:
		[x_min, x_max] = [x_max, x_min]

	if y_min > y_max:
		[y_min, y_max] = [y_max, y_min]

	return [x_min, y_min, x_max, y_max]


def get_cover_bbox(bbox1, bbox2):
	if bbox1 and bbox2:
		return [
			bbox1[0] if bbox1[0] < bbox2[0] else bbox2[0],
			bbox1[1] if bbox1[1] < bbox2[1] else bbox2[1],
			bbox1[2] if bbox1[2] > bbox2[2] else bbox2[2],
			bbox1[3] if bbox1[3] > bbox2[3] else bbox2[3],
		]
	elif bbox1:
		return bbox1
	elif bbox2:
		return bbox2


def get_mbtiles_bbox_from_tiles(cur, scheme="tms"):
	cur.execute("SELECT zoom_level, MIN (tile_column), MAX (tile_column), MIN (tile_row), MAX (tile_row) FROM tiles GROUP BY zoom_level;")
	rows = cur.fetchall()
	if not rows:
		raise ValueError("No row found")

	z, x_min, x_max, y_min, y_max = rows[0]
	bbox = get_bbox_from_tiles(x_min, y_min, x_max, y_max, z, scheme)

	for row in rows[1:]:
		z, x_min, x_max, y_min, y_max = row

		bbox = get_cover_bbox(bbox, get_bbox_from_tiles(x_min, y_min, x_max, y_max, z, scheme))

	bbox[0] = limit_value(bbox[0], -MAX_LON, MAX_LON)
	bbox[2] = limit_value(bbox[2], -MAX_LON, MAX_LON)
	bbox[1] = limit_value(bbox[1], -MAX_LAT, MAX_LAT)
	bbox[3] = limit_value(bbox[3], -MAX_LAT, MAX_LAT)

	return bbox


def get_mbtiles_center_from_bbox_and_zoom(bbox, minzoom, maxzoom):
	return [(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2, math.floor((minzoom + maxzoom) / 2)]


def get_mbtiles_zoom_level_from_tiles(cur, min):
	if min:
		cur.execute("SELECT MIN (zoom_level) FROM tiles;")
	else:
		cur.execute("SELECT MAX (zoom_level) FROM tiles;")

	row = cur.fetchone()
	if not row:
		raise ValueError("No row found")

	return row[0]


def get_mbtiles_format_from_tiles(cur):
	cur.execute("SELECT tile_data FROM tiles LIMIT 1;")
	row = cur.fetchone()
	if not row:
		raise ValueError("No row found")

	if row[0].startswith(b"\x89PNG\r\n\x1a\n"):
		return "png"
	elif row[0].startswith(b"\xff\xd8"):
		return "jpeg"
	elif row[0].startswith((b"GIF87a", b"GIF89a")):
		return "gif"
	elif row[0].startswith(b"RIFF") and row[0][8:12] == b"WEBP":
		return "webp"
	else:
		return "pbf"


def optimize_connect(cur):
	cur.execute("PRAGMA journal_mode = OFF;")
	cur.execute("PRAGMA synchronous = OFF;")
	cur.execute("PRAGMA foreign_keys = OFF;")
	cur.execute("PRAGMA encoding = 'UTF-8';")
	cur.execute("PRAGMA page_size = 65536;")


def ensure_schema(cur, metadata_index, tiles_index, extra_info):
	cur.execute("CREATE TABLE IF NOT EXISTS metadata (name TEXT NOT NULL, value TEXT NOT NULL, UNIQUE (name));")
	cur.execute("CREATE TABLE IF NOT EXISTS tiles (zoom_level INTEGER NOT NULL, tile_column INTEGER NOT NULL, tile_row INTEGER NOT NULL, tile_data BLOB NOT NULL, UNIQUE (zoom_level, tile_column, tile_row));")

	if metadata_index:
		cur.execute("CREATE UNIQUE INDEX IF NOT EXISTS metadata_index ON metadata (name);")

	if tiles_index:
		cur.execute("CREATE UNIQUE INDEX IF NOT EXISTS tiles_index ON tiles (zoom_level, tile_column, tile_row);")

	if extra_info:
		cur.execute("PRAGMA table_info (tiles);")
		rows = cur.fetchall()
		if rows:
			cols = { row[1] for row in rows }
		else:
			cols = {}

		if "hash" not in cols:
			cur.execute("ALTER TABLE tiles ADD COLUMN hash TEXT;")

		if "created" not in cols:
			cur.execute("ALTER TABLE tiles ADD COLUMN created BIGINT;")

		conn.create_function("MD5", 1, lambda blob: hashlib.md5(blob).hexdigest())


def merge_tiles(conn, cur, inputs, merge_strategy, extra_info):
	created = int(time.time() * 1000)

	final_bbox = None
	final_minzoom = None
	final_maxzoom = None
	insert_query = None
	select_query = None

	if merge_strategy == "overwrite":
		if extra_info:
			insert_query = f"INSERT OR REPLACE INTO tiles (zoom_level, tile_column, tile_row, tile_data, hash, created) SELECT zoom_level, tile_column, tile_row, tile_data, MD5 (tile_data), {created} FROM src.tiles;"
		else:
			insert_query = "INSERT OR REPLACE INTO tiles (zoom_level, tile_column, tile_row, tile_data) SELECT zoom_level, tile_column, tile_row, tile_data FROM src.tiles;"
	elif merge_strategy == "keep":
		if extra_info:
			insert_query = f"INSERT OR IGNORE INTO tiles (zoom_level, tile_column, tile_row, tile_data, hash, created) SELECT zoom_level, tile_column, tile_row, tile_data, MD5 (tile_data), {created} FROM src.tiles;"
		else:
			insert_query = "INSERT OR IGNORE INTO tiles (zoom_level, tile_column, tile_row, tile_data) SELECT zoom_level, tile_column, tile_row, tile_data FROM src.tiles;"
	else:
		if extra_info:
			insert_query = "INSERT OR REPLACE INTO tiles (zoom_level, tile_column, tile_row, tile_data, hash, created) VALUES (?, ?, ?, ?, ?, ?);"
		else:
			insert_query = "INSERT OR REPLACE INTO tiles (zoom_level, tile_column, tile_row, tile_data) VALUES (?, ?, ?, ?);"
			select_query = "SELECT src.rowid, src.zoom_level, src.tile_column, src.tile_row, src.tile_data AS new_data, dst.tile_data AS old_data FROM src.tiles src LEFT JOIN tiles dst ON dst.zoom_level = src.zoom_level AND dst.tile_column = src.tile_column AND dst.tile_row = src.tile_row WHERE src.rowid > ? ORDER BY src.rowid LIMIT ?;"

	for path in inputs:
		LOGGER.info(f"Merging {path}...")

		src_conn = sqlite3.connect(path)
		src_cur = src_conn.cursor()

		src_cur.execute("SELECT name, value FROM metadata;")
		src_rows = cur.fetchall()
		if rows:
			src_metadata = { src_row[0]: src_row[1] for src_row in src_rows }
		else:
			src_metadata = {}

		if "bounds" in src_metadata:
			parts = src_metadata["bounds"].split(",")

			if len(parts) != 4:
				raise ValueError("Invalid bounds format")

			bbox = [float(x) for x in parts]
		else:
			bbox = get_mbtiles_bbox_from_tiles(src_cur, "tms")

		if "minzoom" in src_metadata:
			minzoom = int(src_metadata["minzoom"])
		else:
			minzoom = get_mbtiles_zoom_level_from_tiles(src_cur, True)

		if "maxzoom" in src_metadata:
			maxzoom = int(src_metadata["maxzoom"])
		else:
			maxzoom = get_mbtiles_zoom_level_from_tiles(src_cur, False)

		final_bbox = get_cover_bbox(final_bbox, bbox)
		final_minzoom = limit_value(minzoom, None, final_minzoom)
		final_maxzoom = limit_value(maxzoom, final_minzoom, None)

		last_rowid = 0
		format = get_mbtiles_format_from_tiles(src_cur)

		src_conn.close()

		cur.execute("ATTACH DATABASE ? AS src;", (str(path),))

		conn.execute("BEGIN;")

		if select_query:
			if format == "png" or format == "webp":
				while True:
					cur.execute(select_query, (last_rowid, args.batch_size))
					rows = cur.fetchall()
					if not rows:
						break

					for rowid, z, x, y, new_data, old_data in rows:
						if old_data:
							out = io.BytesIO()

							Image.alpha_composite(
								Image.open(io.BytesIO(old_data)),
								Image.open(io.BytesIO(new_data)),
							).save(
								out,
								format=format.upper(),
								optimize=True,
								compress_level=9,
								quality=100,
							)

							merged = out.getvalue()
						else:
							merged = new_data

						if extra_info:
							cur.execute(insert_query, (z, x, y, merged, hashlib.md5(merged).hexdigest(), created))
						else:
							cur.execute(insert_query, (z, x, y, merged))

						last_rowid = rowid
			else:
				raise ValueError(f"Strategy '{merge_strategy}' is not support with format '{format}'")
		else:
			cur.execute(insert_query)

		conn.execute("COMMIT;")

		cur.execute("DETACH DATABASE src;")

	return {
		"bbox": final_bbox,
		"minzoom": final_minzoom,
		"maxzoom": final_maxzoom
	}


def update_metadata(conn, cur, metadata_json, merge_info):
	final_metadata = {}

	if metadata_json:
		try:
			user_meta = json.loads(metadata_json)
			if not isinstance(user_meta, dict):
				raise ValueError("Metadata JSON must be an object")

			final_metadata.update(user_meta)
		except json.JSONDecodeError as e:
			raise ValueError(f"Invalid JSON metadata: {e}")

	cur.execute("SELECT name, value FROM metadata;")
	rows = cur.fetchall()
	if rows:
		metadata = { row[0]: row[1] for row in rows }
	else:
		metadata = {}

	if "name" not in final_metadata:
		if "name" in metadata:
			final_metadata["name"] = metadata["name"]
		else:
			final_metadata["name"] = "Unknown"

	if "description" not in final_metadata:
		if "description" in metadata:
			final_metadata["description"] = metadata["description"]
		else:
			final_metadata["description"] = final_metadata["name"]

	if "attribution" not in final_metadata:
		if "attribution" in metadata:
			final_metadata["attribution"] = metadata["attribution"]
		else:
			final_metadata["attribution"] = "<b>Viettel HighTech</b>"

	if "version" not in final_metadata:
		if "version" in metadata:
			final_metadata["version"] = metadata["version"]
		else:
			final_metadata["version"] = "1.0.0"

	if "type" not in final_metadata:
		if "type" in metadata:
			final_metadata["type"] = metadata["type"]
		else:
			final_metadata["type"] = "overlay"

	if "format" not in final_metadata:
		if "format" in metadata:
			final_metadata["format"] = metadata["format"]
		else:
			final_metadata["format"] = get_mbtiles_format_from_tiles(cur)

	if "minzoom" not in final_metadata:
		if merge_info and merge_info["minzoom"] is not None:
			final_metadata["minzoom"] = merge_info["minzoom"]
		else:
			final_metadata["minzoom"] = get_mbtiles_zoom_level_from_tiles(cur, True)

	if "maxzoom" not in final_metadata:
		if merge_info and merge_info["maxzoom"] is not None:
			final_metadata["maxzoom"] = merge_info["maxzoom"]
		else:
			final_metadata["maxzoom"] = get_mbtiles_zoom_level_from_tiles(cur, False)

	if "bounds" not in final_metadata:
		if merge_info and merge_info["bbox"] is not None:
			bbox = merge_info["bbox"]
		else:
			bbox = get_mbtiles_bbox_from_tiles(cur, "tms")

		final_metadata["bounds"] = ",".join(str(x) for x in bbox)

	if "center" not in final_metadata:
		final_metadata["center"] = ','.join(str(x) for x in get_mbtiles_center_from_bbox_and_zoom(bbox, int(final_metadata["minzoom"]), int(final_metadata["maxzoom"])))

	insert_query = "INSERT INTO metadata (name, value) VALUES (?, ?) ON CONFLICT (name) DO UPDATE SET value = excluded.value;"

	conn.execute("BEGIN;")

	for key, value in final_metadata.items():
		cur.execute(insert_query, (key, value))

	conn.execute("COMMIT;")


if __name__ == "__main__":
	signal.signal(signal.SIGINT, lambda sig, frame: (
		LOGGER.info('Received "SIGINT" signal. Exitting...'),

		sys.exit(0)
	))

	signal.signal(signal.SIGTERM, lambda sig, frame: (
		LOGGER.info('Received "SIGTERM" signal. Exitting...'),

		sys.exit(0)
	))

	parser = argparse.ArgumentParser(description="Extract MBTiles")
	parser.add_argument("-o", "--output", required=True, type=Path, help="Output MBTiles")
	parser.add_argument("-i", "--inputs", required=False, nargs="+", type=Path, help="Input MBTileses")
	parser.add_argument("-b", "--batch-size", required=False, type=int, default=1000, help="Batch size")
	parser.add_argument("-mi", "--metadata-index", required=False, action="store_true", help="Create metadata unique index")
	parser.add_argument("-ti", "--tiles-index", required=False, action="store_true", help="Create tiles unique index")
	parser.add_argument("-e", "--extra-info", required=False, action="store_true", help="Create extra info")
	parser.add_argument("-mj", "--metadata-json", required=False, type=str, help="Metadata JSON string to update metadata table")
	parser.add_argument("-ms", "--merge-strategy", required=False, choices=["overwrite", "keep", "merge"], default="overwrite", help="Merge strategy")
	parser.add_argument("-c", "--compression", required=False, action="store_true", help="Enable compression for output MBTiles")

	args = parser.parse_args()

	LOGGER.info(f"Extract MBTiles {args.output}...")

	t0 = time.time()

	conn = sqlite3.connect(args.output)
	cur = conn.cursor()

	optimize_connect(cur)

	ensure_schema(cur, args.metadata_index, args.tiles_index, args.extra_info)

	if args.inputs:
		LOGGER.info(f"Merge MBTiles {args.output}...")

		merge_info = merge_tiles(conn, cur, args.inputs, args.merge_strategy, args.extra_info)

	LOGGER.info(f"Update MBTiles metadata {args.output}...")

	update_metadata(conn, cur, args.metadata_json, merge_info)

	conn.close()

	LOGGER.info(f"Extracted MBTiles {args.output} in {time.time() - t0:.2f}s!")
