#!/usr/bin/env python3

from multiprocessing import Pool
from pathlib import Path
from PIL import Image
import subprocess
import argparse
import sqlite3
import hashlib
import logging
import signal
import time
import math
import json
import sys
import io
import os


# ========= LOGS =========
LOGGER = logging.getLogger(__name__)
handler = logging.StreamHandler()
handler.setFormatter(logging.Formatter("%(asctime)s %(levelname)5s: %(message)s"))
LOGGER.addHandler(handler)
LOGGER.setLevel(logging.INFO)


SPHERICAL_RADIUS = 6378137.0
MAX_LON = 180
MAX_LAT = 85.051129


os.environ["GDAL_NUM_THREADS"] = "ALL_CPUS"

def parse_bounds(bounds):
	parts = bounds.split(",")

	if len(parts) != 4:
		raise ValueError("Invalid bounds format")

	return [float(x) for x in parts]


def encode_bounds(bounds):
	return ",".join(str(x) for x in bounds)


def limit_value(value, min = None, max = None):
	if min is not None and value < min:
		value = min

	if max is not None and value > max:
		value = max

	return value


def lonlat4326_to_xy3857(lon, lat):
	return limit_value(lon, -MAX_LON, MAX_LON) * math.pi / 180 * SPHERICAL_RADIUS, math.log(math.tan(math.pi * (limit_value(lat, -MAX_LAT, MAX_LAT) + 90) / 360)) * SPHERICAL_RADIUS


def xy3857_to_lonlat4326(x, y):
	return limit_value(x / SPHERICAL_RADIUS * 180 / math.pi, -MAX_LON, MAX_LON), limit_value(math.atan(math.sinh(y / SPHERICAL_RADIUS)) * 180 / math.pi, -MAX_LAT, MAX_LAT)


def get_xyz_from_lonlatz(lon, lat, z, scheme="tms"):
	max_tile = 1 << z

	x = (0.5 + limit_value(lon, -MAX_LON, MAX_LON) / 360) * max_tile
	y = (0.5 - math.log(math.tan(math.pi * (limit_value(lat, -MAX_LAT, MAX_LAT) + 90) / 360)) / (2 * math.pi)) * max_tile

	if scheme == "tms":
		y = max_tile - y

	return [limit_value(math.floor(x), 0, max_tile - 1), limit_value(math.floor(y), 0, max_tile - 1), z]


def get_lonlat_from_xyz(x, y, z, position, scheme="tms"):
	max_tile = 1 << z

	if scheme == "tms":
		y = max_tile - 1 - y

	if position == "center":
		x += 0.5
		y += 0.5
	elif position == "bottom-right":
		x += 1
		y += 1

	return [360 * (x / max_tile - 0.5), (360 * math.atan(math.exp(math.pi * (1 - 2 * y / max_tile))) / math.pi) - 90]


def get_bbox_from_tiles(x_min, y_min, x_max, y_max, z, scheme = "tms"):
	[lon_min, lat_max] = get_lonlat_from_xyz(x_min, y_min, z, "top-left", scheme)
	[lon_max, lat_min] = get_lonlat_from_xyz(x_max, y_max, z, "bottom-right", scheme)

	if lon_min > lon_max:
		[lon_min, lon_max] = [lon_max, lon_min]

	if lat_min > lat_max:
		[lat_min, lat_max] = [lat_max, lat_min]

	return [lon_min, lat_min, lon_max, lat_max]


def get_tiles_from_bbox(bbox, z, scheme = "tms"):
	[x_min, y_min] = get_xyz_from_lonlatz(bbox[0], bbox[3], z, scheme)
	[x_max, y_max] = get_xyz_from_lonlatz(bbox[2], bbox[1], z, scheme)

	if x_min > x_max:
		[x_min, x_max] = [x_max, x_min]

	if y_min > y_max:
		[y_min, y_max] = [y_max, y_min]

	return [x_min, y_min, x_max, y_max]


def get_cover_bbox(bbox1, bbox2):
	if bbox1 and bbox2:
		return [
			bbox1[0] if bbox1[0] < bbox2[0] else bbox2[0],
			bbox1[1] if bbox1[1] < bbox2[1] else bbox2[1],
			bbox1[2] if bbox1[2] > bbox2[2] else bbox2[2],
			bbox1[3] if bbox1[3] > bbox2[3] else bbox2[3],
		]
	elif bbox1:
		return bbox1
	elif bbox2:
		return bbox2


def get_mbtiles_bbox_from_tiles(cur, scheme="tms"):
	cur.execute("SELECT zoom_level, MIN (tile_column), MAX (tile_column), MIN (tile_row), MAX (tile_row) FROM tiles GROUP BY zoom_level;")
	rows = cur.fetchall()
	if not rows:
		raise ValueError("No row found")

	z, x_min, x_max, y_min, y_max = rows[0]
	bbox = get_bbox_from_tiles(x_min, y_min, x_max, y_max, z, scheme)

	for row in rows[1:]:
		z, x_min, x_max, y_min, y_max = row

		bbox = get_cover_bbox(bbox, get_bbox_from_tiles(x_min, y_min, x_max, y_max, z, scheme))

	bbox[0] = limit_value(bbox[0], -MAX_LON, MAX_LON)
	bbox[2] = limit_value(bbox[2], -MAX_LON, MAX_LON)
	bbox[1] = limit_value(bbox[1], -MAX_LAT, MAX_LAT)
	bbox[3] = limit_value(bbox[3], -MAX_LAT, MAX_LAT)

	return bbox


def get_mbtiles_center_from_bbox_and_zoom(bbox, minzoom, maxzoom):
	return [(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2, math.floor((minzoom + maxzoom) / 2)]


def get_mbtiles_zoom_level_from_tiles(cur, min):
	if min:
		cur.execute("SELECT MIN (zoom_level) FROM tiles;")
	else:
		cur.execute("SELECT MAX (zoom_level) FROM tiles;")

	row = cur.fetchone()
	if not row:
		raise ValueError("No row found")

	return row[0]


def get_mbtiles_format_from_tiles(cur):
	cur.execute("SELECT tile_data FROM tiles LIMIT 1;")
	row = cur.fetchone()
	if not row:
		raise ValueError("No row found")

	if row[0].startswith(b"\x89PNG\r\n\x1a\n"):
		return "png"
	elif row[0].startswith(b"\xff\xd8"):
		return "jpeg"
	elif row[0].startswith((b"GIF87a", b"GIF89a")):
		return "gif"
	elif row[0].startswith(b"RIFF") and row[0][8:12] == b"WEBP":
		return "webp"
	else:
		return "pbf"


def connect(file_path):
	conn = sqlite3.connect(file_path)
	cur = conn.cursor()

	cur.execute("PRAGMA journal_mode = OFF;")
	cur.execute("PRAGMA synchronous = OFF;")
	cur.execute("PRAGMA foreign_keys = OFF;")
	cur.execute("PRAGMA encoding = 'UTF-8';")
	cur.execute("PRAGMA page_size = 65536;")

	cur.execute("CREATE TABLE IF NOT EXISTS metadata (name TEXT NOT NULL, value TEXT NOT NULL, UNIQUE (name));")
	cur.execute("CREATE TABLE IF NOT EXISTS tiles (zoom_level INTEGER NOT NULL, tile_column INTEGER NOT NULL, tile_row INTEGER NOT NULL, tile_data BLOB NOT NULL, UNIQUE (zoom_level, tile_column, tile_row));")

	return conn, cur


def create_unique_indexes(cur, metadata_index, tiles_index):
	def has_unique_index(table_name, column_names):
		cur.execute(f"PRAGMA index_list ({table_name});")
		indexes = cur.fetchall()
		if not indexes:
			return False

		for _, index_name, is_unique, *_ in indexes:
			if not is_unique:
				continue

			cur.execute(f"PRAGMA index_info ({index_name});")
			columns = cur.fetchall()
			if not columns:
				continue

			index_cols = [col[2] for col in columns]
			if index_cols == column_names:
				return True

		return False

	if metadata_index and not has_unique_index("metadata", ["name"]):
		cur.execute("CREATE UNIQUE INDEX metadata_index ON metadata (name);")

	if tiles_index and not has_unique_index("tiles", ["zoom_level", "tile_column", "tile_row"]):
		cur.execute("CREATE UNIQUE INDEX tiles_index ON tiles (zoom_level, tile_column, tile_row);")


def composite_worker(args):
	z, x, y, new_data, old_data, save_args = args

	if not old_data:
		return z, x, y, new_data

	out = io.BytesIO()

	Image.alpha_composite(Image.open(io.BytesIO(old_data)), Image.open(io.BytesIO(new_data))).save(out, **save_args)

	return z, x, y, out.getvalue()


def merge_tiles(conn, cur, inputs, merge_strategy, compression, workers, batch):
	final_bounds = None
	final_minzoom = None
	final_maxzoom = None
	insert_query = None
	select_query = None

	if merge_strategy == "overwrite":
		insert_query = "INSERT OR REPLACE INTO tiles (zoom_level, tile_column, tile_row, tile_data) SELECT zoom_level, tile_column, tile_row, tile_data FROM src.tiles;"
	elif merge_strategy == "keep":
		insert_query = "INSERT OR IGNORE INTO tiles (zoom_level, tile_column, tile_row, tile_data) SELECT zoom_level, tile_column, tile_row, tile_data FROM src.tiles;"
	else:
		insert_query = "INSERT OR REPLACE INTO tiles (zoom_level, tile_column, tile_row, tile_data) VALUES (?, ?, ?, ?);"
		select_query = "SELECT src.rowid, src.zoom_level, src.tile_column, src.tile_row, src.tile_data AS new_data, dst.tile_data AS old_data FROM src.tiles src LEFT JOIN tiles dst ON dst.zoom_level = src.zoom_level AND dst.tile_column = src.tile_column AND dst.tile_row = src.tile_row WHERE src.rowid > ? ORDER BY src.rowid LIMIT ?;"

	for path in inputs:
		LOGGER.info(f"Merging {path}...")

		src_conn = sqlite3.connect(path)
		src_cur = src_conn.cursor()

		src_cur.execute("SELECT name, value FROM metadata;")
		src_rows = src_cur.fetchall()
		if src_rows:
			src_metadata = { src_row[0]: src_row[1] for src_row in src_rows }
		else:
			src_metadata = {}

		if "bounds" in src_metadata:
			bbox = parse_bounds(src_metadata["bounds"])
		else:
			bbox = get_mbtiles_bbox_from_tiles(src_cur, "tms")

		if "minzoom" in src_metadata:
			minzoom = int(src_metadata["minzoom"])
		else:
			minzoom = get_mbtiles_zoom_level_from_tiles(src_cur, True)

		if "maxzoom" in src_metadata:
			maxzoom = int(src_metadata["maxzoom"])
		else:
			maxzoom = get_mbtiles_zoom_level_from_tiles(src_cur, False)

		final_bounds = get_cover_bbox(final_bounds, bbox)
		final_minzoom = limit_value(minzoom, None, final_minzoom)
		final_maxzoom = limit_value(maxzoom, final_maxzoom, None)

		last_rowid = 0
		format = get_mbtiles_format_from_tiles(src_cur)

		src_conn.close()

		cur.execute("ATTACH DATABASE ? AS src;", (str(path),))

		conn.execute("BEGIN;")

		if select_query:
			if format == "png" or format == "webp":
				save_args = {
					"format": format,
					"compress_level": 9 if compression else None,
					"quality": 100,
				}

				workers = max(1, workers)

				# ========= SINGLE PROCESS =========
				if workers == 1:
					while True:
						cur.execute(select_query, (last_rowid, batch))
						rows = cur.fetchall()
						if not rows:
							break

						for _, z, x, y, new_data, old_data in rows:
							if old_data:
								out = io.BytesIO()

								Image.alpha_composite(Image.open(io.BytesIO(old_data)), Image.open(io.BytesIO(new_data))).save(out, **save_args)

								merged = out.getvalue()
							else:
								merged = new_data

							cur.execute(insert_query, (z, x, y, merged))

						last_rowid = rows[-1][0]
				# ========= MULTI PROCESS =========
				else:
					try:
						pool = Pool(workers)

						while True:
							cur.execute(select_query, (last_rowid, batch))
							rows = cur.fetchall()
							if not rows:
								break

							for z, x, y, merged in pool.imap_unordered(composite_worker, [(z, x, y, new_data, old_data, save_args) for _, z, x, y, new_data, old_data in rows]):
								cur.execute(insert_query, (z, x, y, merged))

							last_rowid = rows[-1][0]
					finally:
						pool.close()
						pool.join()
			else:
				raise ValueError(f"Strategy '{merge_strategy}' is not support with format '{format}'")
		else:
			cur.execute(insert_query)

		conn.execute("COMMIT;")

		cur.execute("DETACH DATABASE src;")

	return {
		"bounds": final_bounds,
		"minzoom": final_minzoom,
		"maxzoom": final_maxzoom,
	}


def update_metadata(conn, cur, metadata_json, merge_info):
	final_metadata = {}

	cur.execute("SELECT name, value FROM metadata;")
	rows = cur.fetchall()
	if rows:
		final_metadata.update({ row[0]: row[1] for row in rows })

	if merge_info is not None:
		if "bounds" in final_metadata:
			final_metadata["bounds"] = encode_bounds(get_cover_bbox(parse_bounds(final_metadata["bounds"]), merge_info["bounds"]))
		else:
			final_metadata["bounds"] = encode_bounds(merge_info["bounds"])

		if "minzoom" in final_metadata:
			final_metadata["minzoom"] = min(final_metadata["minzoom"], merge_info["minzoom"])
		else:
			final_metadata["minzoom"] = str(merge_info["minzoom"])

		if "maxzoom" in final_metadata:
			final_metadata["maxzoom"] = max(final_metadata["maxzoom"], merge_info["maxzoom"])
		else:
			final_metadata["maxzoom"] = str(merge_info["maxzoom"])

	if metadata_json:
		try:
			user_meta = json.loads(metadata_json)
			if not isinstance(user_meta, dict):
				raise ValueError("Metadata JSON must be an object")

			final_metadata.update(user_meta)
		except json.JSONDecodeError as e:
			raise ValueError(f"Invalid JSON metadata: {e}")

	if "name" not in final_metadata:
		final_metadata["name"] = "Unknown"

	if "description" not in final_metadata:
		final_metadata["description"] = final_metadata["name"]

	if "attribution" not in final_metadata:
		final_metadata["attribution"] = "<b>Viettel HighTech</b>"

	if "version" not in final_metadata:
		final_metadata["version"] = "1.0.0"

	if "type" not in final_metadata:
		final_metadata["type"] = "overlay"

	if "format" not in final_metadata:
		final_metadata["format"] = get_mbtiles_format_from_tiles(cur)

	if "minzoom" not in final_metadata:
		final_metadata["minzoom"] = str(get_mbtiles_zoom_level_from_tiles(cur, True))

	if "maxzoom" not in final_metadata:
		final_metadata["maxzoom"] = str(get_mbtiles_zoom_level_from_tiles(cur, False))

	if "bounds" not in final_metadata:
		bbox = get_mbtiles_bbox_from_tiles(cur, "tms")

		final_metadata["bounds"] = encode_bounds(bbox)
	else:
		bbox = parse_bounds(parts)

	if "center" not in final_metadata:
		final_metadata["center"] = ",".join(str(x) for x in get_mbtiles_center_from_bbox_and_zoom(bbox, int(final_metadata["minzoom"]), int(final_metadata["maxzoom"])))

	insert_query = "INSERT INTO metadata (name, value) VALUES (?, ?) ON CONFLICT (name) DO UPDATE SET value = excluded.value;"

	conn.execute("BEGIN;")

	for key, value in final_metadata.items():
		cur.execute(insert_query, (key, value))

	conn.execute("COMMIT;")


def create_overview(file_path, resampling):
	opts = ["gdaladdo"]

	if resampling:
		opts = opts + ["-r", resampling]

	opts.append(file_path)

	subprocess.run(opts, check=True)


def update_hash_and_created(conn, cur, batch):
	def has_column(column):
		cur.execute("PRAGMA table_info (tiles);")
		rows = cur.fetchall()
		if rows:
			if column in { row[1] for row in rows }:
				return True
			else:
				return False
		else:
			raise ValueError("Cannot get 'tiles' table info")

	if not has_column("hash"):
		cur.execute("ALTER TABLE tiles ADD COLUMN hash TEXT;")

	if not has_column("created"):
		cur.execute("ALTER TABLE tiles ADD COLUMN created BIGINT;")

	created = int(time.time() * 1000)

	select_query = "SELECT rowid, zoom_level, tile_column, tile_row, tile_data FROM tiles WHERE rowid > ? ORDER BY rowid LIMIT ?;"
	update_query = "UPDATE tiles SET hash = ?, created = ? WHERE zoom_level = ? AND tile_column = ? AND tile_row = ?;"

	last_rowid = 0

	conn.execute("BEGIN;")

	while True:
		cur.execute(select_query, (last_rowid, batch))

		rows = cur.fetchall()
		if not rows:
			break

		for rowid, z, x, y, tile_data in rows:
			cur.execute(update_query, (hashlib.md5(tile_data).hexdigest(), created, z, x, y))

			last_rowid = rowid

	conn.execute("COMMIT;")


if __name__ == "__main__":
	signal.signal(signal.SIGINT, lambda sig, frame: (
		LOGGER.info("Received 'SIGINT' signal. Exitting..."),

		sys.exit(0)
	))

	signal.signal(signal.SIGTERM, lambda sig, frame: (
		LOGGER.info("Received 'SIGTERM' signal. Exitting..."),

		sys.exit(0)
	))

	parser = argparse.ArgumentParser(description="Extract MBTiles")
	parser.add_argument("-o", "--output", required=True, type=Path, help="Output MBTiles")
	parser.add_argument("-i", "--inputs", required=False, nargs="+", type=Path, help="Input MBTileses")
	parser.add_argument("-w", "--workers", required=False, type=int, default=1, help="Number of worker processes")
	parser.add_argument("-b", "--batch", required=False, type=int, default=1000, help="Batch size")
	parser.add_argument("-mi", "--metadata-index", required=False, action="store_true", help="Create metadata unique index")
	parser.add_argument("-ti", "--tiles-index", required=False, action="store_true", help="Create tiles unique index")
	parser.add_argument("-e", "--extra-info", required=False, action="store_true", help="Create extra info in tiles table (hash and created columns)")
	parser.add_argument("-mj", "--metadata-json", required=False, type=str, help="Metadata JSON string to update metadata table")
	parser.add_argument("-ms", "--merge-strategy", required=False, choices=["overwrite", "keep", "merge"], default="overwrite", help="Merge strategy")
	parser.add_argument("-c", "--compression", required=False, action="store_true", help="Enable compression tile for output MBTiles")
	parser.add_argument("-ovr", "--overview", required=False, action="store_true", help="Create overview for output MBTiles")
	parser.add_argument("-r", "--resampling", required=False, choices=["nearest", "average", "rms", "gauss", "bilinear", "cubic", "cubicspline", "lanczos", "average_magphase", "mode"], default="nearest", help="Resampling method to create overview")

	args = parser.parse_args()

	LOGGER.info(f"Extract MBTiles {args.output}...")

	t0 = time.time()

	conn, cur = connect(args.output)

	if args.tiles_index or args.tiles_index:
		LOGGER.info(f"Create indexes MBTiles {args.output}...")

		create_unique_indexes(cur, args.metadata_index, args.tiles_index)

	merge_info = None
	if args.inputs:
		LOGGER.info(f"Merge MBTiles {args.output}...")

		merge_info = merge_tiles(conn, cur, args.inputs, args.merge_strategy, args.compression, args.workers, args.batch)

	if args.overview:
		LOGGER.info(f"Create overview MBTiles {args.output}...")

		conn.close()

		create_overview(args.output, args.resampling)

		conn, cur = connect(args.output)

	if args.extra_info:
		LOGGER.info(f"Update MBTiles hash and created {args.output}...")

		update_hash_and_created(conn, cur, args.batch)

	LOGGER.info(f"Update MBTiles metadata {args.output}...")

	update_metadata(conn, cur, args.metadata_json, merge_info)

	conn.close()

	LOGGER.info(f"Extracted MBTiles {args.output} in {time.time() - t0:.2f}s!")
