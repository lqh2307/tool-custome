#!/usr/bin/env python3

from multiprocessing import Pool
from pathlib import Path
from PIL import Image
import subprocess
import argparse
import sqlite3
import hashlib
import logging
import signal
import time
import math
import json
import sys
import io
import os


# ========= LOGS =========
LOGGER = logging.getLogger(__name__)
handler = logging.StreamHandler()
handler.setFormatter(logging.Formatter("%(asctime)s %(levelname)5s: %(message)s"))
LOGGER.addHandler(handler)
LOGGER.setLevel(logging.INFO)


# ========= CONSTANTS =========
SPHERICAL_RADIUS = 6378137.0
MAX_LON = 180
MAX_LAT = 85.051129


# ========= EVIRONMENTS =========
os.environ["GDAL_NUM_THREADS"] = "ALL_CPUS"


# ========= UTILS =========
def parse_bounds(bounds):
	parts = bounds.split(",")

	if len(parts) != 4:
		raise ValueError("Invalid bounds format")

	if parts[0] >= parts[2] or parts[0] < -180 or parts[2] > 180 or parts[1] >= parts[3] or parts[1] < -90 or parts[3] > 90:
		raise ValueError("Invalid bounds")

	return [float(x) for x in parts]


def encode_array(bounds):
	return ",".join(str(x) for x in bounds)


def limit_value(value, min = None, max = None):
	if min is not None and value < min:
		value = min

	if max is not None and value > max:
		value = max

	return value


def lonlat4326_to_xy3857(lon, lat):
	return limit_value(lon, -MAX_LON, MAX_LON) * math.pi / 180 * SPHERICAL_RADIUS, math.log(math.tan(math.pi * (limit_value(lat, -MAX_LAT, MAX_LAT) + 90) / 360)) * SPHERICAL_RADIUS


def xy3857_to_lonlat4326(x, y):
	return limit_value(x / SPHERICAL_RADIUS * 180 / math.pi, -MAX_LON, MAX_LON), limit_value(math.atan(math.sinh(y / SPHERICAL_RADIUS)) * 180 / math.pi, -MAX_LAT, MAX_LAT)


def get_xyz_from_lonlatz(lon, lat, z, scheme="tms"):
	max_tile = 1 << z

	x = (0.5 + limit_value(lon, -MAX_LON, MAX_LON) / 360) * max_tile
	y = (0.5 - math.log(math.tan(math.pi * (limit_value(lat, -MAX_LAT, MAX_LAT) + 90) / 360)) / (2 * math.pi)) * max_tile

	if scheme == "tms":
		y = max_tile - y

	return [limit_value(math.floor(x), 0, max_tile - 1), limit_value(math.floor(y), 0, max_tile - 1), z]


def get_lonlat_from_xyz(x, y, z, position, scheme="tms"):
	max_tile = 1 << z

	if scheme == "tms":
		y = max_tile - 1 - y

	if position == "center":
		x += 0.5
		y += 0.5
	elif position == "bottom-right":
		x += 1
		y += 1

	return [360 * (x / max_tile - 0.5), (360 * math.atan(math.exp(math.pi * (1 - 2 * y / max_tile))) / math.pi) - 90]


def get_bbox_from_tiles(x_min, y_min, x_max, y_max, z, scheme = "tms"):
	[lon_min, lat_max] = get_lonlat_from_xyz(x_min, y_min, z, "top-left", scheme)
	[lon_max, lat_min] = get_lonlat_from_xyz(x_max, y_max, z, "bottom-right", scheme)

	if lon_min > lon_max:
		[lon_min, lon_max] = [lon_max, lon_min]

	if lat_min > lat_max:
		[lat_min, lat_max] = [lat_max, lat_min]

	return [lon_min, lat_min, lon_max, lat_max]


def get_tiles_from_bbox(bbox, z, scheme = "tms"):
	[x_min, y_min] = get_xyz_from_lonlatz(bbox[0], bbox[3], z, scheme)
	[x_max, y_max] = get_xyz_from_lonlatz(bbox[2], bbox[1], z, scheme)

	if x_min > x_max:
		[x_min, x_max] = [x_max, x_min]

	if y_min > y_max:
		[y_min, y_max] = [y_max, y_min]

	return [x_min, y_min, x_max, y_max]


def get_cover_bbox(bbox1, bbox2):
	if bbox1 and bbox2:
		return [
			bbox1[0] if bbox1[0] < bbox2[0] else bbox2[0],
			bbox1[1] if bbox1[1] < bbox2[1] else bbox2[1],
			bbox1[2] if bbox1[2] > bbox2[2] else bbox2[2],
			bbox1[3] if bbox1[3] > bbox2[3] else bbox2[3],
		]
	elif bbox1:
		return bbox1
	elif bbox2:
		return bbox2

	return None


def get_intersect_bbox(bbox1, bbox2):
	if bbox1 and bbox2:
		minLon = bbox1[0] if bbox1[0] > bbox2[0] else bbox2[0]
		minLat = bbox1[1] if bbox1[1] > bbox2[1] else bbox2[1]
		maxLon = bbox1[2] if bbox1[2] < bbox2[2] else bbox2[2]
		maxLat = bbox1[3] if bbox1[3] < bbox2[3] else bbox2[3]

		if (minLon >= maxLon or minLat >= maxLat):
			return None
	elif bbox1:
		return bbox1
	elif bbox2:
		return bbox2

	return None


# ========= MBTILES =========
def get_mbtiles_bounds_from_tiles(cur, scheme="tms"):
	cur.execute("SELECT zoom_level, MIN (tile_column), MAX (tile_column), MIN (tile_row), MAX (tile_row) FROM tiles GROUP BY zoom_level;")
	rows = cur.fetchall()
	if not rows:
		return None

	z, x_min, x_max, y_min, y_max = rows[0]
	bounds = get_bbox_from_tiles(x_min, y_min, x_max, y_max, z, scheme)

	for row in rows[1:]:
		z, x_min, x_max, y_min, y_max = row

		bounds = get_cover_bbox(bounds, get_bbox_from_tiles(x_min, y_min, x_max, y_max, z, scheme))

	bounds[0] = limit_value(bounds[0], -MAX_LON, MAX_LON)
	bounds[2] = limit_value(bounds[2], -MAX_LON, MAX_LON)
	bounds[1] = limit_value(bounds[1], -MAX_LAT, MAX_LAT)
	bounds[3] = limit_value(bounds[3], -MAX_LAT, MAX_LAT)

	return bounds

def get_mbtiles_zoom_level_from_tiles(cur, min):
	if min:
		cur.execute("SELECT MIN (zoom_level) FROM tiles;")
	else:
		cur.execute("SELECT MAX (zoom_level) FROM tiles;")

	row = cur.fetchone()
	if not row:
		return None

	return row[0]


def get_mbtiles_format_from_tiles(cur):
	cur.execute("SELECT tile_data FROM tiles LIMIT 1;")
	row = cur.fetchone()
	if not row:
		return None

	if row[0].startswith(b"\x89PNG\r\n\x1a\n"):
		return "png"
	elif row[0].startswith(b"\xff\xd8"):
		return "jpeg"
	elif row[0].startswith((b"GIF87a", b"GIF89a")):
		return "gif"
	elif row[0].startswith(b"RIFF") and row[0][8:12] == b"WEBP":
		return "webp"
	else:
		return "pbf"


def connect(file_path):
	conn = sqlite3.connect(file_path)
	cur = conn.cursor()

	cur.execute("PRAGMA journal_mode = OFF;")
	cur.execute("PRAGMA synchronous = OFF;")
	cur.execute("PRAGMA foreign_keys = OFF;")
	cur.execute("PRAGMA encoding = 'UTF-8';")
	cur.execute("PRAGMA page_size = 65536;")

	cur.execute("CREATE TABLE IF NOT EXISTS metadata (name TEXT NOT NULL, value TEXT NOT NULL, UNIQUE (name));")
	cur.execute("CREATE TABLE IF NOT EXISTS tiles (zoom_level INTEGER NOT NULL, tile_column INTEGER NOT NULL, tile_row INTEGER NOT NULL, tile_data BLOB NOT NULL, UNIQUE (zoom_level, tile_column, tile_row));")

	return conn, cur


def create_unique_indexes(cur, metadata_index, tiles_index):
	def has_unique_index(table_name, column_names):
		cur.execute(f"PRAGMA index_list ({table_name});")
		indexes = cur.fetchall()
		if not indexes:
			return False

		for _, index_name, is_unique, *_ in indexes:
			if not is_unique:
				continue

			cur.execute(f"PRAGMA index_info ({index_name});")
			columns = cur.fetchall()
			if not columns:
				continue

			index_cols = [col[2] for col in columns]
			if index_cols == column_names:
				return True

		return False

	if metadata_index and not has_unique_index("metadata", ["name"]):
		cur.execute("CREATE UNIQUE INDEX metadata_index ON metadata (name);")

	if tiles_index and not has_unique_index("tiles", ["zoom_level", "tile_column", "tile_row"]):
		cur.execute("CREATE UNIQUE INDEX tiles_index ON tiles (zoom_level, tile_column, tile_row);")


def composite_image(args):
	z, x, y, new_data, old_data, save_args = args

	if not old_data:
		return z, x, y, new_data

	out = io.BytesIO()

	Image.alpha_composite(Image.open(io.BytesIO(old_data)), Image.open(io.BytesIO(new_data))).save(out, **save_args)

	return z, x, y, out.getvalue()


def merge_tiles(conn, cur, inputs, bbox, merge_strategy, compression, workers, batch_size):
	insert_query = None
	select_query = None
	if merge_strategy == "overwrite":
		insert_query = "INSERT OR REPLACE INTO tiles (zoom_level, tile_column, tile_row, tile_data) SELECT zoom_level, tile_column, tile_row, tile_data FROM src.tiles;"
	elif merge_strategy == "keep":
		insert_query = "INSERT OR IGNORE INTO tiles (zoom_level, tile_column, tile_row, tile_data) SELECT zoom_level, tile_column, tile_row, tile_data FROM src.tiles;"
	else:
		insert_query = "INSERT OR REPLACE INTO tiles (zoom_level, tile_column, tile_row, tile_data) VALUES (?, ?, ?, ?);"
		select_query = "SELECT src.rowid, src.zoom_level, src.tile_column, src.tile_row, src.tile_data AS new_data, dst.tile_data AS old_data FROM src.tiles src LEFT JOIN tiles dst ON dst.zoom_level = src.zoom_level AND dst.tile_column = src.tile_column AND dst.tile_row = src.tile_row WHERE src.rowid > ? ORDER BY src.rowid LIMIT ?;"

	final_bounds = None

	for path in inputs:
		LOGGER.info(f"Merging {path}...")

		bounds = None
		last_rowid = 0

		src_conn = sqlite3.connect(path)
		src_cur = src_conn.cursor()

		src_cur.execute("SELECT value FROM metadata WHERE name = ?;", )
		src_row = src_cur.fetchone()
		if src_row:
			bounds = parse_bounds(src_row[0])
		else:
			bounds = get_mbtiles_bounds_from_tiles(src_cur, "tms")

		if bbox and get_intersect_bbox(bbox, bounds) is None:
			continue

		format = get_mbtiles_format_from_tiles(src_cur)

		src_conn.close()

		final_bounds = get_cover_bbox(final_bounds, bounds)

		cur.execute("ATTACH DATABASE ? AS src;", (str(path),))

		conn.execute("BEGIN;")

		if select_query:
			if format == "png" or format == "webp":
				save_args = {
					"format": format,
					"compress_level": 9 if compression else None,
					"quality": 100,
				}

				# ========= SINGLE PROCESS =========
				if workers == 1:
					while True:
						cur.execute(select_query, (last_rowid, batch_size))
						rows = cur.fetchall()
						if not rows:
							break

						for _, z, x, y, new_data, old_data in rows:
							merged = composite_image((z, x, y, new_data, old_data, save_args))

							cur.execute(insert_query, (z, x, y, merged))

						last_rowid = rows[-1][0]
				# ========= MULTI PROCESS =========
				else:
					try:
						pool = Pool(workers)

						while True:
							cur.execute(select_query, (last_rowid, batch_size))
							rows = cur.fetchall()
							if not rows:
								break

							for z, x, y, merged in pool.imap_unordered(composite_image, [(z, x, y, new_data, old_data, save_args) for _, z, x, y, new_data, old_data in rows]):
								cur.execute(insert_query, (z, x, y, merged))

							last_rowid = rows[-1][0]
					finally:
						pool.close()
						pool.join()
			else:
				raise ValueError(f"Strategy '{merge_strategy}' is not support with format '{format}'")
		else:
			cur.execute(insert_query)

		conn.execute("COMMIT;")

		cur.execute("DETACH DATABASE src;")

	return get_intersect_bbox(bbox, final_bounds)


def update_metadata(conn, cur, metadata_json, merge_bounds):
	final_metadata = {}

	cur.execute("SELECT name, value FROM metadata;")
	rows = cur.fetchall()
	if rows:
		final_metadata.update({ row[0]: row[1] for row in rows })

	if merge_bounds:
		if "bounds" in final_metadata:
			final_metadata["bounds"] = encode_array(get_cover_bbox(parse_bounds(final_metadata["bounds"]), merge_bounds))
		else:
			final_metadata["bounds"] = encode_array(merge_bounds)

	if metadata_json:
		final_metadata.update(metadata_json)

	if "name" not in final_metadata:
		final_metadata["name"] = "Unknown"

	if "description" not in final_metadata:
		final_metadata["description"] = final_metadata["name"]

	if "attribution" not in final_metadata:
		final_metadata["attribution"] = "<b>Viettel HighTech</b>"

	if "version" not in final_metadata:
		final_metadata["version"] = "1.0.0"

	if "type" not in final_metadata:
		final_metadata["type"] = "overlay"

	if "format" not in final_metadata:
		format = get_mbtiles_format_from_tiles(cur)
		if format is not None:
			final_metadata["format"] = format

	minzoom = None
	if "minzoom" not in final_metadata:
		minzoom = get_mbtiles_zoom_level_from_tiles(cur, True)
		if minzoom is not None:
			final_metadata["minzoom"] = str(minzoom)

	maxzoom = None
	if "maxzoom" not in final_metadata:
		maxzoom = get_mbtiles_zoom_level_from_tiles(cur, False)
		if maxzoom is not None:
			final_metadata["maxzoom"] = str(maxzoom)

	bounds = None
	if "bounds" not in final_metadata:
		bounds = get_mbtiles_bounds_from_tiles(cur, "tms")
		if bounds:
			final_metadata["bounds"] = encode_array(bounds)
	else:
		bounds = parse_bounds(final_metadata["bounds"])

	if "center" not in final_metadata:
		if bounds and minzoom is not None and maxzoom is not None:
			lon = (bounds[0] + bounds[2]) / 2
			lat = (bounds[1] + bounds[3]) / 2
			zoom = math.floor((minzoom + maxzoom) / 2)

			final_metadata["center"] = encode_array([lon, lat, zoom])

	insert_query = "INSERT INTO metadata (name, value) VALUES (?, ?) ON CONFLICT (name) DO UPDATE SET value = excluded.value;"

	conn.execute("BEGIN;")

	for key, value in final_metadata.items():
		cur.execute(insert_query, (key, value))

	conn.execute("COMMIT;")


def create_overview(file_path, resampling):
	opts = ["gdaladdo"]

	if resampling:
		opts = opts + ["-r", resampling]

	opts.append(file_path)

	subprocess.run(opts, check=True)


def update_hash_and_created(conn, cur, batch_size):
	def has_column(column):
		cur.execute("PRAGMA table_info (tiles);")
		rows = cur.fetchall()
		if rows:
			if column in { row[1] for row in rows }:
				return True
			else:
				return False
		else:
			raise ValueError("Cannot get 'tiles' table info")

	if not has_column("hash"):
		cur.execute("ALTER TABLE tiles ADD COLUMN hash TEXT;")

	if not has_column("created"):
		cur.execute("ALTER TABLE tiles ADD COLUMN created BIGINT;")

	created = int(time.time() * 1000)

	select_query = "SELECT rowid, zoom_level, tile_column, tile_row, tile_data FROM tiles WHERE rowid > ? ORDER BY rowid LIMIT ?;"
	update_query = "UPDATE tiles SET hash = ?, created = ? WHERE zoom_level = ? AND tile_column = ? AND tile_row = ?;"

	last_rowid = 0

	conn.execute("BEGIN;")

	while True:
		cur.execute(select_query, (last_rowid, batch_size))

		rows = cur.fetchall()
		if not rows:
			break

		for rowid, z, x, y, tile_data in rows:
			cur.execute(update_query, (hashlib.md5(tile_data).hexdigest(), created, z, x, y))

			last_rowid = rowid

	conn.execute("COMMIT;")


if __name__ == "__main__":
	signal.signal(signal.SIGINT, lambda sig, frame: (
		LOGGER.info("Received 'SIGINT' signal. Exitting..."),

		sys.exit(0)
	))

	signal.signal(signal.SIGTERM, lambda sig, frame: (
		LOGGER.info("Received 'SIGTERM' signal. Exitting..."),

		sys.exit(0)
	))

	parser = argparse.ArgumentParser(description="Extract MBTiles")
	parser.add_argument("-o", "--output", required=True, type=Path, help="Output MBTiles")
	parser.add_argument("-i", "--inputs", required=False, nargs="+", type=Path, help="Input MBTileses")
	parser.add_argument("-w", "--workers", required=False, type=int, default=1, help="Number of worker processes")
	parser.add_argument("-bz", "--batch_size", required=False, type=int, default=1000, help="Batch size")
	parser.add_argument("-mi", "--metadata-index", required=False, action="store_true", help="Create metadata unique index")
	parser.add_argument("-ti", "--tiles-index", required=False, action="store_true", help="Create tiles unique index")
	parser.add_argument("-ei", "--extra-info", required=False, action="store_true", help="Create extra info in tiles table (hash and created columns)")
	parser.add_argument("-mj", "--metadata-json", required=False, type=str, help="Metadata JSON string to update metadata table")
	parser.add_argument("-ms", "--merge-strategy", required=False, choices=["overwrite", "keep", "merge"], default="overwrite", help="Merge strategy")
	parser.add_argument("-c", "--compression", required=False, action="store_true", help="Enable compression tile for output MBTiles")
	parser.add_argument("-ovr", "--overview", required=False, action="store_true", help="Create overview for output MBTiles")
	parser.add_argument("-r", "--resampling", required=False, choices=["nearest", "average", "rms", "gauss", "bilinear", "cubic", "cubicspline", "lanczos", "average_magphase", "mode"], default="nearest", help="Resampling method to create overview")
	parser.add_argument("-b", "--bbox", required=False, type=str, help="Only merge tiles covered by bounding box coordinates in the format 'minX,minY,maxX,maxY'")

	args = parser.parse_args()

	# Parse and validate bbox
	bbox = None
	if args.bbox:
		try:
			bbox = parse_bounds(args.bbox)
		except ValueError as e:
			raise ValueError(f"Invalid BBox coordinates: ${e}")

	# Parse and validate metadata json
	metadata_json = None
	if args.metadata_json:
		try:
			metadata_json = json.loads(args.metadata_json)
			if not isinstance(metadata_json, dict):
				raise ValueError("Metadata JSON must be an object")
		except Exception as e:
			raise ValueError(f"Invalid Metadata JSON: {e}")

	LOGGER.info(f"Extract MBTiles {args.output}...")

	t0 = time.time()

	conn, cur = connect(args.output)

	if args.metadata_index or args.tiles_index:
		LOGGER.info(f"Create indexes MBTiles {args.output}...")

		create_unique_indexes(cur, args.metadata_index, args.tiles_index)

	merge_bounds = None
	if args.inputs:
		LOGGER.info(f"Merge MBTiles {args.output}...")

		merge_bounds = merge_tiles(conn, cur, args.inputs, bbox, args.merge_strategy, args.compression, max(1, args.worker), args.batch_size)

	if args.overview:
		LOGGER.info(f"Create overview MBTiles {args.output}...")

		conn.close()

		create_overview(args.output, args.resampling)

		conn, cur = connect(args.output)

	if args.extra_info:
		LOGGER.info(f"Update MBTiles hash and created {args.output}...")

		update_hash_and_created(conn, cur, args.batch_size)

	LOGGER.info(f"Update MBTiles metadata {args.output}...")

	update_metadata(conn, cur, metadata_json, merge_bounds)

	conn.close()

	LOGGER.info(f"Extracted MBTiles {args.output} in {time.time() - t0:.2f}s!")
